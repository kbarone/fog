{"cells": [{"cell_type": "code", "execution_count": 77, "id": "ca6dfc75-ff82-4481-b469-6bbebb3dd19d", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom google.cloud import storage\n\nimport matplotlib.pyplot as plt\n%matplotlib inline"}, {"cell_type": "code", "execution_count": 78, "id": "d84c522a-b335-4e97-9228-fcbb7e519438", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://hub-hub-msca-bdp-dphub-student-ksarussi-m.c.msca-bdp-student-ap.internal:36359\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f224d7ad580>"}, "execution_count": 78, "metadata": {}, "output_type": "execute_result"}], "source": "spark = SparkSession.builder.appName('ParquetExample').getOrCreate()\nspark"}, {"cell_type": "code", "execution_count": 79, "id": "88a727ba-feda-4fa4-855a-821399a1f5c0", "metadata": {}, "outputs": [], "source": "client = storage.Client()\n# https://cloud.google.com/storage/docs/samples/storage-list-files#storage_list_files-python"}, {"cell_type": "code", "execution_count": 82, "id": "d68e6c8c-1ba7-4a1e-aefc-b70ab3996916", "metadata": {}, "outputs": [], "source": "parquet_bucket_path = \"parkinsons_data/unlabeled\"\nparquet_bucket_name = \"msca-bdp-student-gcs\"\nfull_path = \"msca-bdp-student-gcs/parkinsons_data/unlabeled\""}, {"cell_type": "code", "execution_count": 83, "id": "08555c2b-78c6-437d-b60b-09faaf5c5f72", "metadata": {}, "outputs": [], "source": "blobs = client.list_blobs(parquet_bucket_name, prefix=parquet_bucket_path)"}, {"cell_type": "code", "execution_count": 84, "id": "878e5874-0389-41bd-bf60-b9816e6a398a", "metadata": {}, "outputs": [{"data": {"text/plain": "<google.api_core.page_iterator.HTTPIterator at 0x7f2244ba3880>"}, "execution_count": 84, "metadata": {}, "output_type": "execute_result"}], "source": "blobs"}, {"cell_type": "code", "execution_count": 85, "id": "4681c1a0-bb54-4774-bcc2-5a99fa138928", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "file name: 00c4c9313d\nfile name: 07a96f89ec\nfile name: 0d1bc672a8\nfile name: 0e333c9833\nfile name: 164adaed7b\nfile name: 17e0c0dc86\nfile name: 1c3719ea59\nfile name: 1cf80df2d6\nfile name: 24016102f2\nfile name: 276630050d\nfile name: 28e6c306ba\nfile name: 2caa348298\nfile name: 32bdbc35a0\nfile name: 3ae6b0f79f\nfile name: 3bd159ded0\nfile name: 3f51a63612\nfile name: 3fc03f01ed\nfile name: 3fe2624b51\nfile name: 40bf6c162f\nfile name: 418a1ca2c1\nfile name: 43ac46d679\nfile name: 48081794eb\nfile name: 48b636e0f5\nfile name: 4b84027351\nfile name: 4e44a97a85\nfile name: 52fd07ea27\nfile name: 5535c94fc9\nfile name: 57741bad42\nfile name: 5bf570bb7b\nfile name: 5e13d48878\nfile name: 6e0303484e\nfile name: 6ed2f109c3\nfile name: 74f1e1e0ba\nfile name: 7ab610bb34\nfile name: 831c13620e\nfile name: 88f67f91db\nfile name: 8959244e1c\nfile name: 8ca674a988\nfile name: 924e997065\nfile name: 93abd37fee\nfile name: 96f57b4a40\nfile name: 9da3e3dc66\nfile name: 9fb7805d99\nfile name: a213c90b02\nfile name: b15168b788\nfile name: b18354d4aa\nfile name: b1ba59e5f0\nfile name: ba970e7e9a\nfile name: baac585916\nfile name: bbe8b2d4b8\nfile name: bd6f65a4d2\nfile name: be948ac8ad\nfile name: bfb2732959\nfile name: c0201855a9\nfile name: c3e0d7aad0\nfile name: cb8698473f\nfile name: d37397f832\nfile name: d9d96400ea\nfile name: e2d103a18e\nfile name: e328c64434\nfile name: e658b0aa3d\nfile name: ed0a487f20\nfile name: ef1db3ca64\nfile name: f16c5cda55\nfile name: f3fa3bf72d\n"}], "source": "from pyspark.sql.functions import lit\n\ncombined_df = None\nfor blob in blobs:\n    file_name = blob.name.split(\"/\")[-1].split(\".\")[0]\n    print(\"file name:\", file_name)\n    df = spark.read.parquet(f\"gs://{parquet_bucket_name}/{blob.name}\")\n    # add file name as column - https://sparkbyexamples.com/pyspark/pyspark-lit-add-literal-constant/\n    df = df.withColumn(\"file_name\", lit(file_name))\n    if combined_df is None:\n        combined_df = df\n    else:\n#         https://www.geeksforgeeks.org/how-to-union-multiple-dataframe-in-pyspark/\n        combined_df = combined_df.union(df)\n\n   "}, {"cell_type": "code", "execution_count": 73, "id": "cc0f0d15-fe44-47e4-bf15-4046e611f172", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Time: long (nullable = true)\n |-- AccV: double (nullable = true)\n |-- AccML: double (nullable = true)\n |-- AccAP: double (nullable = true)\n |-- file_name: string (nullable = false)\n\n"}], "source": "combined_df.printSchema()\n"}, {"cell_type": "code", "execution_count": 72, "id": "27e6bacc-cb6e-463b-a22a-9b33ebf49c67", "metadata": {}, "outputs": [{"data": {"text/plain": "550"}, "execution_count": 72, "metadata": {}, "output_type": "execute_result"}], "source": "num_partitions = combined_df.rdd.getNumPartitions()\nnum_partitions"}, {"cell_type": "code", "execution_count": 75, "id": "47be502b-36b5-4b36-bd6c-388b38ac4653", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "60479669"}, "execution_count": 75, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import max\n\nmax_time = df.select(max(\"Time\")).collect()[0][0]\nmax_time\n\n## need to do more robustness checking on some sample files \n# to make sure the join actually worked. Maybe run this by file_id,\n# there's a decent amount of variation in amount of data provided by person\n# (some files are way bigger than others by person)\n"}, {"cell_type": "code", "execution_count": 76, "id": "0c40c9a6-e394-4fc2-b909-91e729fdca71", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "4.20992160917169"}, "execution_count": 76, "metadata": {}, "output_type": "execute_result"}], "source": "max_AccV = df.select(max(\"AccV\")).collect()[0][0]\nmax_AccV"}, {"cell_type": "code", "execution_count": 87, "id": "6ef73c3f-a019-450d-b8b1-d64338d7b4c6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "'f3fa3bf72d'"}, "execution_count": 87, "metadata": {}, "output_type": "execute_result"}], "source": "max_pesron = df.select(max(\"file_name\")).collect()[0][0]\nmax_pesron"}, {"cell_type": "code", "execution_count": null, "id": "de509775-f3e9-42f6-8ef3-431ce2fa561e", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}