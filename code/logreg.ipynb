{"cells": [{"cell_type": "code", "execution_count": 1, "id": "89567b2b-68f4-4ebd-aa5e-7d1401371546", "metadata": {}, "outputs": [], "source": "import os \n#from fog.code.utils.utils import *\n\nimport pyspark.sql.functions as F\n#from pyspark.sql.types import StringType, BooleanType, IntegerType, FloatType, DateType\nfrom pyspark.sql import SparkSession\nfrom google.cloud import storage\n\n# import Spark stuff\nfrom pyspark import SparkFiles\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"}, {"cell_type": "code", "execution_count": 2, "id": "cd9beaa1-9aac-4238-8f49-db14b8717484", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.appName('defog-labeled').getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "2d5f20b6-f31e-4bb0-b5c5-49f4a3a8d28e", "metadata": {}, "outputs": [], "source": "defog_path = \"parkinsons_data/train/processed/defog_tasks_lagging\"\ntop_bucket_name = \"msca-bdp-student-gcs\""}, {"cell_type": "code", "execution_count": 4, "id": "632ce81d-4238-4a66-bbcd-d91c48f4b1cc", "metadata": {}, "outputs": [], "source": "def feed_files(top_bucket_name, prefix, suffix):\n    client = storage.Client()\n    blobs = client.list_blobs(top_bucket_name, prefix=prefix)\n\n    processed = None\n\n    for i, blob in enumerate(blobs):\n        print(blob.name)\n        if blob.name.endswith(suffix):\n            \n            if suffix == \".parquet\":\n                df = spark.read.parquet(f\"gs://{top_bucket_name}/{blob.name}\")\n            elif suffix == \".csv\":\n                df = spark.read.csv(f\"gs://{top_bucket_name}/{blob.name}\")\n            if processed is None:\n                processed = df\n            else:\n                processed = processed.union(df)\n    return processed\n\n"}, {"cell_type": "code", "execution_count": 5, "id": "1132ec70-5ead-42e4-a98a-02d686c9193c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "parkinsons_data/train/processed/defog_tasks_lagging/\nparkinsons_data/train/processed/defog_tasks_lagging/_SUCCESS\nparkinsons_data/train/processed/defog_tasks_lagging/part-00000-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "parkinsons_data/train/processed/defog_tasks_lagging/part-00001-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00002-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00003-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "parkinsons_data/train/processed/defog_tasks_lagging/part-00004-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00005-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00006-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00007-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00008-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00009-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00010-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00011-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00012-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00013-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00014-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00015-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00016-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00017-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00018-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00019-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00020-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00021-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00022-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00023-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00024-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00025-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00026-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00027-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\nparkinsons_data/train/processed/defog_tasks_lagging/part-00028-eae2acef-cda6-49a5-b43f-ded309b7bad5-c000.snappy.parquet\n"}], "source": "defog = feed_files(top_bucket_name, defog_path, \".parquet\")"}, {"cell_type": "code", "execution_count": 6, "id": "287a778e-9b69-4b06-b075-f49a04cfe0a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Subject: string (nullable = true)\n |-- Id: string (nullable = true)\n |-- Time: float (nullable = true)\n |-- AccV: float (nullable = true)\n |-- AccML: float (nullable = true)\n |-- AccAP: float (nullable = true)\n |-- StartHesitation: integer (nullable = true)\n |-- Turn: integer (nullable = true)\n |-- Walking: integer (nullable = true)\n |-- Valid: boolean (nullable = true)\n |-- Task: boolean (nullable = true)\n |-- SourceDefog: integer (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Sex: string (nullable = true)\n |-- YearsSinceDx: integer (nullable = true)\n |-- UPDRSIII_On: integer (nullable = true)\n |-- UPDRSIII_Off: integer (nullable = true)\n |-- NFOGQ: integer (nullable = true)\n |-- TimeSeconds: double (nullable = true)\n |-- Begin: double (nullable = true)\n |-- End: double (nullable = true)\n |-- TaskType: string (nullable = true)\n |-- MB9: integer (nullable = true)\n |-- Rest1: integer (nullable = true)\n |-- MB6-L: integer (nullable = true)\n |-- MB6-R: integer (nullable = true)\n |-- Turning-C: integer (nullable = true)\n |-- MB2a: integer (nullable = true)\n |-- MB3-L: integer (nullable = true)\n |-- MB12: integer (nullable = true)\n |-- MB5: integer (nullable = true)\n |-- MB3-R: integer (nullable = true)\n |-- MB13: integer (nullable = true)\n |-- TUG-DT: integer (nullable = true)\n |-- Turning-ST: integer (nullable = true)\n |-- TUG-ST: integer (nullable = true)\n |-- 4MW-C: integer (nullable = true)\n |-- Hotspot2: integer (nullable = true)\n |-- MB6: integer (nullable = true)\n |-- TUG-C: integer (nullable = true)\n |-- 4MW: integer (nullable = true)\n |-- Hotspot1-C: integer (nullable = true)\n |-- Hotspot2-C: integer (nullable = true)\n |-- MB8: integer (nullable = true)\n |-- Hotspot1: integer (nullable = true)\n |-- MB4: integer (nullable = true)\n |-- MB1: integer (nullable = true)\n |-- MB7: integer (nullable = true)\n |-- Rest2: integer (nullable = true)\n |-- MB2b: integer (nullable = true)\n |-- MB10: integer (nullable = true)\n |-- Turning-DT: integer (nullable = true)\n |-- MB11: integer (nullable = true)\n |-- target: integer (nullable = true)\n |-- features: vector (nullable = true)\n |-- standardized: vector (nullable = true)\n |-- prediction_67: integer (nullable = true)\n |-- prediction4: integer (nullable = true)\n |-- AccV_lag1: float (nullable = true)\n |-- AccV_lag2: float (nullable = true)\n |-- AccV_lag3: float (nullable = true)\n |-- AccV_lag4: float (nullable = true)\n |-- AccV_lag5: float (nullable = true)\n |-- AccV_lag6: float (nullable = true)\n |-- AccV_lag7: float (nullable = true)\n |-- AccV_lag8: float (nullable = true)\n |-- AccV_lag9: float (nullable = true)\n |-- AccV_lag10: float (nullable = true)\n |-- AccML_lag1: float (nullable = true)\n |-- AccML_lag2: float (nullable = true)\n |-- AccML_lag3: float (nullable = true)\n |-- AccML_lag4: float (nullable = true)\n |-- AccML_lag5: float (nullable = true)\n |-- AccML_lag6: float (nullable = true)\n |-- AccML_lag7: float (nullable = true)\n |-- AccML_lag8: float (nullable = true)\n |-- AccML_lag9: float (nullable = true)\n |-- AccML_lag10: float (nullable = true)\n |-- AccAP_lag1: float (nullable = true)\n |-- AccAP_lag2: float (nullable = true)\n |-- AccAP_lag3: float (nullable = true)\n |-- AccAP_lag4: float (nullable = true)\n |-- AccAP_lag5: float (nullable = true)\n |-- AccAP_lag6: float (nullable = true)\n |-- AccAP_lag7: float (nullable = true)\n |-- AccAP_lag8: float (nullable = true)\n |-- AccAP_lag9: float (nullable = true)\n |-- AccAP_lag10: float (nullable = true)\n\n"}], "source": "# drop Test, Visit, Medication cols because they were for tdcsfog\ndefog = defog.drop(\"Test\", \"Visit\", \"Medication\")\n\n# keep only when valid = True\ndefog = defog.filter(F.col(\"Valid\")==True).filter(F.col(\"Task\")==True)\n\n# convert Time to float\ndefog = defog.withColumn(\"Time\", F.col(\"Time\").cast(\"float\"))\ndefog.printSchema()"}, {"cell_type": "code", "execution_count": 7, "id": "6445400f-ef05-4d4e-9bc9-877b34e769a8", "metadata": {}, "outputs": [], "source": "# create sex binary cols\ndefog = defog.withColumn('SexInd', \n                  F.when((F.col(\"Sex\") == \"F\"), 1) \\\n                    .when((F.col(\"Sex\") == \"M\"), 0)\n                  )"}, {"cell_type": "code", "execution_count": 8, "id": "e9c27dc8-5abd-460e-918a-2fec0c9d54d2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# convert task types to multilabels\n# keep null tasks for now\nfrom pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol='TaskType', outputCol='TaskTypeInd', handleInvalid=\"keep\")\nmodel = indexer.fit(defog)\ndefog = model.transform(defog)\n\n# use model.labels to see the order of the labels"}, {"cell_type": "code", "execution_count": 9, "id": "bc162102-262e-46e0-9767-2dd8fa3f4c2a", "metadata": {}, "outputs": [], "source": "# remove unnecessary cols\ndefog = defog.drop(\"Subject\", \"Sex\", \"Id\", \"TaskType\")"}, {"cell_type": "code", "execution_count": 10, "id": "80a6285f-2918-4856-b71d-bb6c77ae5a2f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------+-------+\n|target|  count|\n+------+-------+\n|     1|     88|\n|     3|  70521|\n|     2| 414380|\n|     0|3626334|\n+------+-------+\n\n"}], "source": "defog.groupBy(\"target\").count().show()"}, {"cell_type": "code", "execution_count": 20, "id": "3536b3fd-8402-4393-96d3-340af32da47e", "metadata": {}, "outputs": [], "source": "# feature selection\n# ignore standardized for now\ndef vectorize():\n    \"\"\"\n    Creates vectorized dataframe.\n    \"\"\"\n    \"\"\"\n    feature_cols = [\"AccV\", \"AccML\", \"AccAP\", \"TaskTypeInd\",\n        \"AccV_lag1\", \"AccV_lag2\", \"AccV_lag3\", \"AccV_lag4\", \"AccV_lag5\",\n        \"AccV_lag6\", \"AccV_lag7\", \"AccV_lag8\", \"AccV_lag9\", \"AccV_lag10\",\n        \"AccML_lag1\", \"AccML_lag2\", \"AccML_lag3\", \"AccML_lag4\", \"AccML_lag5\",\n        \"AccML_lag6\", \"AccML_lag7\", \"AccML_lag8\", \"AccML_lag9\", \"AccML_lag10\",\n        \"AccAP_lag1\", \"AccAP_lag2\", \"AccAP_lag3\", \"AccAP_lag4\", \"AccAP_lag5\",\n        \"AccAP_lag6\", \"AccAP_lag7\", \"AccAP_lag8\", \"AccAP_lag9\", \"AccAP_lag10\",\n        \"prediction4\"]\n    \"\"\"\n    #feature_cols = ['standardized'] + [\"TaskTypeInd\", \"prediction4\"]\n    label_cols = ['target', 'StartHesitation', 'Turn', 'Walking']\n    feature_cols = [\"AccV\", \"AccML\", \"AccAP\", \"TaskTypeInd\", \"prediction4\"]\n\n    defog_sel = defog.select(feature_cols + label_cols)\n    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid=\"skip\")\n    vectorized = assembler.transform(defog_sel)\n    return vectorized"}, {"cell_type": "code", "execution_count": 23, "id": "b44b8e7f-17f3-4699-8753-e96d4e87fb5b", "metadata": {}, "outputs": [], "source": "def run_logreg(train, test, labelCol):\n    # create model\n    lgr = LogisticRegression(maxIter=10, featuresCol = 'features', labelCol=labelCol)\n\n    # fit model\n    lgrm = lgr.fit(train)\n\n    # make predictions\n    predictions = lgrm.transform(test)\n\n    return predictions\n\ndef evaluate(predictions):\n    evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\")\n\n    print(\"Accuracy\", evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n    print(\"F1\", evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"}, {"cell_type": "markdown", "id": "2fa06aba-491a-4ada-8671-98ed8b78e984", "metadata": {}, "source": "Lag features, not standardized:  \n> Accuracy 0.8811464693942825  \nF1 0.8278086328796053\n\nNo lag, standardized:  \n>Accuracy 0.8818123580030658  \nF1 0.8396314248066408  \n\nNo lag, unstandardized:  \n>Accuracy 0.8819386763598863  \nF1 0.8404164161360337  \n\nUse unstandardized, no lag\n"}, {"cell_type": "code", "execution_count": 24, "id": "86accbc7-0e34-4e37-aca6-74f222f910cd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy 0.8815381570335543\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 184:====================================================>(278 + 1) / 279]\r"}, {"name": "stdout", "output_type": "stream", "text": "F1 0.8398448335102066\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# no lag, unstandardized\nvectorized = vectorize()\ntrain, test = vectorized.randomSplit([0.8, 0.2],0.0)\npredictions = run_logreg(train, test, 'target')\nevaluate(predictions)"}, {"cell_type": "code", "execution_count": 25, "id": "c2c070bb-cc72-4402-8ae5-8d00d74076d2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 186:======================================================>(57 + 1) / 58]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+------+\n|prediction| count|\n+----------+------+\n|       0.0|807749|\n|       3.0|     3|\n|       2.0| 13585|\n+----------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "predictions.select('prediction').groupBy('prediction').count().show()"}, {"cell_type": "markdown", "id": "64264b21-6886-4854-8690-7368c6b143ed", "metadata": {}, "source": "Our algorithm can't predict labels 1 and 3.\nWe will try to treat it as a binary classification problem.  \n\nTry classifying label 1 = Start Hesitation"}, {"cell_type": "code", "execution_count": 26, "id": "a97ea15b-cb5d-41e6-a213-7597e5385254", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---------------+-------+\n|StartHesitation|  count|\n+---------------+-------+\n|              1|     88|\n|              0|4111235|\n+---------------+-------+\n\n"}], "source": "vectorized.groupBy('StartHesitation').count().show()"}, {"cell_type": "code", "execution_count": 28, "id": "86971af7-a79d-4874-a589-644e492df844", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy 0.8815551570007687\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 226:====================================================>(278 + 1) / 279]\r"}, {"name": "stdout", "output_type": "stream", "text": "F1 0.8260608167059249\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# binary target=1\npredictions = run_logreg(train, test, 'StartHesitation')\nevaluate(predictions)"}, {"cell_type": "code", "execution_count": 29, "id": "a5021cdc-65fc-4d89-9a9b-268f8254c6e2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 228:====================================================>(277 + 2) / 279]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+------+\n|prediction| count|\n+----------+------+\n|       0.0|823531|\n+----------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# show predicted labels\npredictions.groupBy(\"prediction\").count().show()"}, {"cell_type": "markdown", "id": "0f17a47d-d395-4443-b3d4-0997d33c2065", "metadata": {}, "source": "Treating label 1 as a binary problem doesn't improve classification. We will try to undersample the data instead."}, {"cell_type": "code", "execution_count": 30, "id": "95ab9f72-1e1c-4ecd-9642-174eacda3c7a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 237:====================================================>(557 + 1) / 558]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------------+-----+\n|StartHesitation|count|\n+---------------+-----+\n|              1|   88|\n|              0|  496|\n+---------------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "def resample(large_dataframe, ratio, class_field, base_class):\n    \"\"\"\n    Resamples non-minority label by a chosen factor\n    \"\"\"\n    pos = large_dataframe.filter(large_dataframe[class_field] != base_class)\n    neg = large_dataframe.filter(large_dataframe[class_field] == base_class)\n    total_pos = pos.count()\n    total_neg = neg.count()\n    fraction=float(total_pos*ratio)/float(total_neg)\n    sampled = neg.sample(False,fraction)\n    \n    return sampled.union(pos)\nsampled = resample(vectorized, 5, 'StartHesitation', 0)\nsampled.select(\"StartHesitation\").groupBy(\"StartHesitation\").count().show()"}, {"cell_type": "code", "execution_count": 31, "id": "12368b8e-7a0f-40e4-8bc0-e66b378dad9f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy 0.7542372881355932\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 272:====================================================>(557 + 1) / 558]\r"}, {"name": "stdout", "output_type": "stream", "text": "F1 0.655858511422255\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# binary target=1, undersampled majority\ntrain, test = sampled.randomSplit([0.8, 0.2],0.0)\npredictions = run_logreg(train, test, 'StartHesitation')\nevaluate(predictions)"}, {"cell_type": "code", "execution_count": 77, "id": "a52d7749-60ff-4519-a5c7-74e9e9418ce7", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/22 01:31:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1975.5 KiB\n[Stage 432:====================================================>(943 + 1) / 944]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----+\n|prediction|count|\n+----------+-----+\n|       0.0|  117|\n|       1.0|   21|\n+----------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "predictions.groupBy(\"prediction\").count().show()"}, {"cell_type": "markdown", "id": "14930568-57fa-4b47-8540-1650e989481e", "metadata": {}, "source": "This is better, but we've had to severely undersample the data."}, {"cell_type": "markdown", "id": "cd9b333c-c971-4253-940c-9fbc7c5f9cb0", "metadata": {}, "source": "Now treat label 3=walking as binary"}, {"cell_type": "code", "execution_count": 32, "id": "784c358d-b61b-4ea8-bb0e-0f9daa5071aa", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 274:======================================================>(57 + 1) / 58]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-------+\n|Walking|  count|\n+-------+-------+\n|      1|  70521|\n|      0|4040802|\n+-------+-------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "vectorized.groupBy('Walking').count().show()"}, {"cell_type": "code", "execution_count": 33, "id": "18cbdffd-2e88-4640-89bc-649be84e2070", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy 0.8815478713005339\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 319:====================================================>(278 + 1) / 279]\r"}, {"name": "stdout", "output_type": "stream", "text": "F1 0.8260571882775194\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# binary target=3\ntrain, test = vectorized.randomSplit([0.8, 0.2],0.0)\npredictions = run_logreg(train, test, 'Walking')\nevaluate(predictions)"}, {"cell_type": "code", "execution_count": 34, "id": "7f4f42e2-9888-4c43-907a-0144dfd1e78f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 321:====================================================>(278 + 1) / 279]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+------+\n|prediction| count|\n+----------+------+\n|       0.0|823525|\n|       1.0|     6|\n+----------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "predictions.groupBy(\"prediction\").count().show()"}, {"cell_type": "markdown", "id": "145f7859-e2bc-4b67-9ea3-4c7e10529911", "metadata": {}, "source": "Now we try to undersampling"}, {"cell_type": "code", "execution_count": 35, "id": "c417e57e-4a17-4717-9b86-bcc86e60c7af", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 332:====================================================>(552 + 6) / 558]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+------+\n|Walking| count|\n+-------+------+\n|      1| 70521|\n|      0|352607|\n+-------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# undersample majority relative to \"Walking\"\nsampled = resample(vectorized, 5, 'Walking', 0)\nsampled.select(\"Walking\").groupBy(\"Walking\").count().show()"}, {"cell_type": "code", "execution_count": 36, "id": "df958948-29e7-4032-b115-542aa53cb725", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy 0.7473675522067754\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 377:====================================================>(557 + 1) / 558]\r"}, {"name": "stdout", "output_type": "stream", "text": "F1 0.6397606656325797\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# binary target=3, undersampled majority\ntrain, test = sampled.randomSplit([0.8, 0.2],0.0)\npredictions = run_logreg(train, test, 'Walking')\nevaluate(predictions)"}, {"cell_type": "code", "execution_count": 37, "id": "a5c737df-02d1-404a-b494-8684b1672a75", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 379:====================================================>(557 + 1) / 558]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----+\n|prediction|count|\n+----------+-----+\n|       0.0|84761|\n|       1.0|   46|\n+----------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "predictions.groupBy(\"prediction\").count().show()"}, {"cell_type": "markdown", "id": "0186dd88-f15c-4ca6-bf1c-a54dc991a467", "metadata": {}, "source": "Even though some models produce higher accuracies and F1 scores, they may not be the most sensitive models because they end up not being able to predict all of our labels. This is likely due to our unbalanced dataset, that has a lot of rows with labels=0, 2 and a relatively small number with labels=1, 3. Some areas to explore in the future could be a combination of undersampling and oversampling, or appying a method such as SMOTE to generate new data points."}, {"cell_type": "code", "execution_count": null, "id": "eb0a9c6a-7f9e-4ce1-a003-33d8352b0982", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}