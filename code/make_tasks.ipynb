{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c28b5f7-56b4-4f97-9bbc-b3afe3b34608",
   "metadata": {},
   "source": [
    "Join tasks data with fog, transform target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a66e2a5-c353-4f44-a848-b186e1e45cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#from utils.utils import *\n",
    "from fog.code.utils.utils import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8df8cd8-eed3-4e26-99f3-072e1a39eb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.minExecutors', '1'),\n",
       " ('spark.dataproc.metrics.listener.metrics.collector.hostname', 'bdp-zm-m'),\n",
       " ('spark.dataproc.sql.parquet.enableFooterCache', 'true'),\n",
       " ('spark.sql.warehouse.dir', 'file:/spark-warehouse'),\n",
       " ('spark.dataproc.sql.joinConditionReorder.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'bdp-zm-m'),\n",
       " ('spark.executor.memory', '5739m'),\n",
       " ('spark.driver.port', '42681'),\n",
       " ('spark.history.fs.logDirectory',\n",
       "  'gs://dataproc-temp-us-central1-635155370842-uzamlpgc/3b0a7a61-e426-4c65-998e-695fa4b5fd84/spark-job-history'),\n",
       " ('spark.yarn.am.memory', '640m'),\n",
       " ('spark.driver.host', 'bdp-zm-m.c.msca-bdp-student-ap.internal'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.concurrent.write.enabled',\n",
       "  'false'),\n",
       " ('spark.dataproc.sql.local.rank.pushdown.enabled', 'true'),\n",
       " ('spark.app.startTime', '1684441511214'),\n",
       " ('spark.executor.instances', '2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.unmanagedAM.enabled', 'true'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '43m'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.extraListeners',\n",
       "  'com.google.cloud.spark.performance.DataprocMetricsListener'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.sql.cbo.joinReorder.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://bdp-zm-m:8088/proxy/application_1684436604634_0012'),\n",
       " ('spark.driver.maxResultSize', '1920m'),\n",
       " ('spark.eventLog.dir',\n",
       "  'gs://dataproc-temp-us-central1-635155370842-uzamlpgc/3b0a7a61-e426-4c65-998e-695fa4b5fd84/spark-job-history'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1684436604634_0012'),\n",
       " ('spark.metrics.namespace',\n",
       "  'app_name:${spark.app.name}.app_id:${spark.app.id}'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.dataproc.sql.optimizer.leftsemijoin.conversion.enabled', 'true'),\n",
       " ('spark.sql.adaptive.enabled', 'true'),\n",
       " ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.hadoop.hive.execution.engine', 'mr'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10000'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.ui.port', '0'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://bdp-zm-m.c.msca-bdp-student-ap.internal:38997'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rpc.message.maxSize', '512'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'application_1684436604634_0012'),\n",
       " ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.driver.memory', '3840m'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.yarn.historyServer.address', 'bdp-zm-m:18080'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.cbo.enabled', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"tasks\").getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5e4c5-6f5e-4d22-833b-a0b5f33a2a4a",
   "metadata": {},
   "source": [
    "### Tasks metadata description  \n",
    "tasks.csv: Task metadata for series in the defog dataset. (Not relevant for the series in the tdcsfog or daily datasets.)    \n",
    "&emsp; 1. Id: The data series where the task was measured.  \n",
    "&emsp; 2. Begin: Time (s) the task began.  \n",
    "&emsp; 3. End: Time (s) the task ended.  \n",
    "&emsp; 4. Task: One of seven tasks types in the DeFOG protocol, described on this page.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bab7a1f-5deb-4cc4-914a-a0bdbc991e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tasks_path = \"gs://msca-bdp-student-gcs/parkinsons_data/tasks.csv\"\n",
    "tasks = spark.read.csv(tasks_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773507f5-d7a0-4801-a390-190b7cd8847a",
   "metadata": {},
   "source": [
    "1. convert times from string to double\n",
    "2. rename Type column to TypeName\n",
    "3. create dummy variables\n",
    "4. merge with fog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ee7d3f-f0b2-4f44-8335-ea5f07b5e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert times to double\n",
    "tasks = tasks.withColumn(\"Begin\",tasks.Begin.cast('double')) \\\n",
    "    .withColumn(\"End\",tasks.End.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00e8c99-2b32-4422-962e-7953b5cbf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename tasks\n",
    "tasks = tasks.withColumnRenamed(\"Task\",\"TaskType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a678a90-d76d-4b4e-a3fb-c43413083f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# count occurrences of each task to compare later\n",
    "task_counts = tasks.groupBy(\"TaskType\").count().sort(F.desc(\"count\")).collect()\n",
    "\n",
    "task_dict ={}\n",
    "for task, count in task_counts:\n",
    "    task_dict[task] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b4551a-2919-4143-8796-efe14de4a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy vars\n",
    "tasks_dummy = create_dummies(tasks, \"TaskType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d0d5ed-f0ab-45b5-977f-eb41bf7e019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 20:26:39 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have retained the original counts: True\n"
     ]
    }
   ],
   "source": [
    "# check that we retained original counts\n",
    "test_count = tasks_dummy.groupBy().sum().collect()\n",
    "test_dict = test_count[0].asDict()\n",
    "for k, v in test_dict.items():\n",
    "    col = k[4:-1]\n",
    "    if col != \"Begin\" and col != \"End\":\n",
    "        if v != task_dict[col]:\n",
    "            print(\"We have retained the original counts:\", False)\n",
    "print(\"We have retained the original counts:\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52aa3a76-d7d8-489f-8ef9-3c7cecbbe293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+--------+---+-----+-----+-----+---------+----+-----+----+---+-----+----+------+----------+------+-----+--------+---+-----+---+----------+----------+---+--------+---+---+---+-----+----+----+----------+----+\n",
      "|        Id| Begin|   End|TaskType|MB9|Rest1|MB6-L|MB6-R|Turning-C|MB2a|MB3-L|MB12|MB5|MB3-R|MB13|TUG-DT|Turning-ST|TUG-ST|4MW-C|Hotspot2|MB6|TUG-C|4MW|Hotspot1-C|Hotspot2-C|MB8|Hotspot1|MB4|MB1|MB7|Rest2|MB2b|MB10|Turning-DT|MB11|\n",
      "+----------+------+------+--------+---+-----+-----+-----+---------+----+-----+----+---+-----+----+------+----------+------+-----+--------+---+-----+---+----------+----------+---+--------+---+---+---+-----+----+----+----------+----+\n",
      "|02ab235146|  10.0|190.48|   Rest1|  0|    1|    0|    0|        0|   0|    0|   0|  0|    0|   0|     0|         0|     0|    0|       0|  0|    0|  0|         0|         0|  0|       0|  0|  0|  0|    0|   0|   0|         0|   0|\n",
      "|02ab235146|211.24|271.56|   Rest2|  0|    0|    0|    0|        0|   0|    0|   0|  0|    0|   0|     0|         0|     0|    0|       0|  0|    0|  0|         0|         0|  0|       0|  0|  0|  0|    1|   0|   0|         0|   0|\n",
      "|02ab235146|505.88| 522.4|     4MW|  0|    0|    0|    0|        0|   0|    0|   0|  0|    0|   0|     0|         0|     0|    0|       0|  0|    0|  1|         0|         0|  0|       0|  0|  0|  0|    0|   0|   0|         0|   0|\n",
      "|02ab235146|577.96|594.64|   4MW-C|  0|    0|    0|    0|        0|   0|    0|   0|  0|    0|   0|     0|         0|     0|    1|       0|  0|    0|  0|         0|         0|  0|       0|  0|  0|  0|    0|   0|   0|         0|   0|\n",
      "|02ab235146|701.32|715.28|     MB1|  0|    0|    0|    0|        0|   0|    0|   0|  0|    0|   0|     0|         0|     0|    0|       0|  0|    0|  0|         0|         0|  0|       0|  0|  1|  0|    0|   0|   0|         0|   0|\n",
      "+----------+------+------+--------+---+-----+-----+-----+---------+----+-----+----+---+-----+----+------+----------+------+-----+--------+---+-----+---+----------+----------+---+--------+---+---+---+-----+----+----+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks_dummy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3904ea77-3980-47b2-8e52-53c01eb20f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - the following file could not be read: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 20:26:53 WARN org.apache.spark.sql.execution.datasources.DataSource: All paths were ignored:\n",
      "  gs://msca-bdp-student-gcs/parkinsons_data/train/processed/defog/_SUCCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - the following file could not be read: defog/_SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - the following file could not be read: defog_tasks/\n"
     ]
    }
   ],
   "source": [
    "# load in fog\n",
    "fog_path = \"parkinsons_data/train/processed/\"\n",
    "fog_files = list_blobs(\"msca-bdp-student-gcs\", string_match=fog_path)\n",
    "\n",
    "fog = feed_files(fog_files, prefix=fog_path, spark_session=spark, file_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e63de5-db44-4523-8e3e-1b79c934fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert fog TimeSeconds to float\n",
    "fog = fog.withColumn(\"TimeSeconds\",fog.TimeSeconds.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb5a950-5122-4108-b297-c0181f25927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join fog with tasks\n",
    "cond = (tasks_dummy.Id == fog.Id) & (fog.TimeSeconds.between(tasks_dummy.Begin, tasks_dummy.End))\n",
    "fog_tasks = fog \\\n",
    "    .join(tasks_dummy, how='left', on=cond)\\\n",
    "    .drop(tasks_dummy.Id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb2f877-38ff-4ed3-9a7b-3d0256d68ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (855264793.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Are counts same after join? -> {fog_tasks.count() == fog.count()}\"\"\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "print(f\"Are counts same after join? -> {fog_tasks.count() == fog.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73d382a7-5f56-48e2-8539-9033c930a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_tasks1 = transform_target(fog_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1cbf0a3-6d9d-4507-9a47-6f3c013a8e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 20:29:32 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "WRITE = False\n",
    "if WRITE:\n",
    "    fog_tasks1.write.format(\"parquet\").mode(\"overwrite\").save(\"gs://msca-bdp-student-gcs/parkinsons_data/train/processed/defog_tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00bfd2e9-4a98-4653-9fff-2b35d6419eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 20:32:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1283.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27051404"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fog_tasks1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a93b0680-b201-4875-a9da-dd6c21b68ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 20:33:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1283.7 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27051404"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fog_tasks.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec4da6a8-f8be-48ab-8d54-897c083b8705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27051404"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fog.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f48c3e0-6cef-43ea-849f-78fe0205824b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fog_tasks1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfog_tasks1\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fog_tasks1' is not defined"
     ]
    }
   ],
   "source": [
    "fog_tasks1.select(\"target\").groupBy('target').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0547b0f-2f27-4d6d-8a7f-c457bb26eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_tasks_path = \"parkinsons_data/train/processed/defog_tasks\"\n",
    "fog_task_files = list_blobs(\"msca-bdp-student-gcs\", string_match=fog_tasks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3642015-fbd1-449b-8ee8-01ca7e918892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
